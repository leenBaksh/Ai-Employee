"""
cloud/health_monitor.py — Cloud Agent Health Monitor (Platinum Tier)

Monitors both Cloud and Local agents by reading their heartbeat signals
from the vault /Signals/ directory. Writes alerts to /Needs_Action/ if
either agent goes offline.

Run alongside cloud_agent.py:
    uv run python cloud/health_monitor.py

Or import and use within an orchestrator.
"""

import json
import time
import logging
import os
from pathlib import Path
from datetime import datetime, timezone

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [HealthMonitor] %(levelname)s: %(message)s",
)
logger = logging.getLogger("HealthMonitor")

VAULT_PATH    = Path(os.getenv("VAULT_PATH", "./AI_Employee_Vault")).resolve()
SIGNALS_DIR   = VAULT_PATH / "Signals"
NEEDS_ACTION  = VAULT_PATH / "Needs_Action"
LOGS_DIR      = VAULT_PATH / "Logs"

# Agent is considered offline if no heartbeat for this many seconds
OFFLINE_THRESHOLD_S = int(os.getenv("HEALTH_OFFLINE_THRESHOLD", "300"))  # 5 minutes
CHECK_INTERVAL_S    = int(os.getenv("HEALTH_CHECK_INTERVAL", "60"))       # 1 minute

KNOWN_AGENTS = ["cloud-01", "local-01"]


def check_agent(agent_id: str) -> dict:
    """Read an agent's latest health signal and return status info."""
    signal_file = SIGNALS_DIR / f"HEALTH_{agent_id}.json"
    if not signal_file.exists():
        return {"agent_id": agent_id, "status": "unknown", "age_s": None}

    try:
        data = json.loads(signal_file.read_text(encoding="utf-8"))
        ts   = datetime.fromisoformat(data["timestamp"])
        age  = (datetime.now(timezone.utc) - ts).total_seconds()
        status = "online" if age < OFFLINE_THRESHOLD_S else "offline"
        return {"agent_id": agent_id, "status": status, "age_s": age, "data": data}
    except Exception as e:
        return {"agent_id": agent_id, "status": "error", "error": str(e), "age_s": None}


def write_offline_alert(agent_id: str, age_s: float):
    """Write an alert to /Needs_Action/ when an agent goes offline."""
    alert_file = NEEDS_ACTION / f"ALERT_agent_offline_{agent_id}.md"
    if alert_file.exists():
        return  # Don't spam alerts
    ts = datetime.now(timezone.utc).isoformat()
    content = f"""---
type: alert
severity: high
source: health_monitor
created: {ts}
agent_offline: {agent_id}
last_seen_seconds_ago: {age_s:.0f}
---

## ⚠️ Agent Offline: {agent_id}

The **{agent_id}** agent has not sent a heartbeat in **{age_s / 60:.1f} minutes**.

## Possible Causes
- Cloud VM rebooted or crashed
- Network connectivity issue
- Process was killed or OOM-killed

## Suggested Actions
- [ ] SSH into the cloud VM and check process: `systemctl status ai-employee-cloud`
- [ ] Check logs: `journalctl -u ai-employee-cloud --since "10 minutes ago"`
- [ ] Restart if needed: `systemctl restart ai-employee-cloud`
- [ ] Verify vault sync is working: check `/Signals/HEALTH_{agent_id}.json`

---
*Generated by Health Monitor · Platinum Tier · {ts}*
"""
    alert_file.write_text(content, encoding="utf-8")
    logger.warning(f"Agent OFFLINE alert written: {agent_id} (last seen {age_s:.0f}s ago)")


def resolve_offline_alert(agent_id: str):
    """Remove offline alert when agent comes back online."""
    alert_file = NEEDS_ACTION / f"ALERT_agent_offline_{agent_id}.md"
    if alert_file.exists():
        done_file = VAULT_PATH / "Done" / alert_file.name
        alert_file.rename(done_file)
        logger.info(f"Agent {agent_id} back online — alert resolved.")


def write_health_report():
    """Write a summary health report to /Signals/HEALTH_REPORT.json."""
    report = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "agents": {}
    }
    for agent_id in KNOWN_AGENTS:
        status = check_agent(agent_id)
        report["agents"][agent_id] = {
            "status":  status["status"],
            "age_s":   status.get("age_s"),
        }
    report_file = SIGNALS_DIR / "HEALTH_REPORT.json"
    SIGNALS_DIR.mkdir(parents=True, exist_ok=True)
    report_file.write_text(json.dumps(report, indent=2), encoding="utf-8")
    return report


def run():
    """Main health monitoring loop."""
    logger.info(f"Health Monitor starting (threshold={OFFLINE_THRESHOLD_S}s, interval={CHECK_INTERVAL_S}s)")
    SIGNALS_DIR.mkdir(parents=True, exist_ok=True)
    NEEDS_ACTION.mkdir(parents=True, exist_ok=True)

    while True:
        try:
            report = write_health_report()
            for agent_id, info in report["agents"].items():
                if info["status"] == "offline":
                    write_offline_alert(agent_id, info["age_s"])
                elif info["status"] == "online":
                    resolve_offline_alert(agent_id)
                logger.info(f"Agent {agent_id}: {info['status']} "
                            f"(last seen {info['age_s']:.0f}s ago)" if info["age_s"] else
                            f"Agent {agent_id}: {info['status']}")
        except Exception as e:
            logger.error(f"Health monitor error: {e}")

        time.sleep(CHECK_INTERVAL_S)


if __name__ == "__main__":
    run()
